{"name":"django-scraper","tagline":"Django application which crawls and downloads online content following instructions","body":"django-scraper\r\n==============\r\n\r\n[![Build Status](https://travis-ci.org/zniper/django-scraper.svg)](https://travis-ci.org/zniper/django-scraper)\r\n[![Coverage Status](https://coveralls.io/repos/zniper/django-scraper/badge.svg?branch=master)](https://coveralls.io/r/zniper/django-scraper?branch=master)\r\n[![Latest Version](https://pypip.in/version/django-scraper/badge.svg)](https://pypi.python.org/pypi/django-scraper/)\r\n\r\n**django-scraper** is a Django application which crawls and downloads online content following configurable instructions.\r\n\r\n* Extract content of given online websites/pages using XPath queries.\r\n* Process can be started from command line (~cron job) or inside Django code \r\n* Automatically browse and download content in related pages, with given depth.\r\n* Support metadata extract along with other content\r\n* Have content refinement rules and black words filtering\r\n* Store and prevent duplication of downloaded content\r\n* Allow changing User Agent\r\n* Support proxy servers\r\n\r\nInstallation\r\n------------\r\nThis application requires some other tools installed first:\r\n    \r\n    lxml\r\n    requests\r\n\r\n**django-scraper** installation can be made using `pip`:\r\n\r\n    pip install django-scraper\r\n    \r\nConfiguration\r\n-------------\r\nIn order to use **django-scraper**, it should be put into `Django` settings as installed application.\r\n    \r\n    INSTALLED_APPS = (\r\n        ...\r\n        'scraper',\r\n    )\r\n\r\nIf `south` is present in current Django project, please use `migrate` command to create database tables. \r\n  \r\n    python manage.py migrate scraper\r\n\r\nOtherwise, please use standard 'syncdb' command\r\n\r\n    python manage.py syncdb\r\n    \r\nThere is also an important configuration value should be added into settings file:\r\n\r\n    CRAW_ROOT = '/path/to/local/storage'\r\n    \r\nUsage\r\n-----\r\nTo start using the application, you should create new `Source` object via admin interface. There, please enter following information:\r\n    \r\n* `url` - URL to the start page of `source` (website, entry list,...)\r\n* `name` - Name of the source\r\n* `link xpath` - XPATH to links of main content page (entries, articles,...)\r\n* `expand rules` - XPATH to url values of next scraping session ~ higher depth\r\n* `crawl depth` - Max depth of scraping session. This relates to expand rules\r\n* `content xpath` - XPATH to the target value of content page (article body,...)\r\n* `content type` - Type of the current `source`\r\n* `meta xpath` - Python dictionary of meta-data information will extracted along the main content\r\n\r\n    *Example:*\r\n        \r\n        {\r\n            'title': '//h1[@class=\"title\"]/text()',\r\n            'keywords': 'keywords': '//meta[@name=\"keywords\"]/@content',\r\n        }\r\n* `extra xpath` - XPATH to additional content that will be downloaded (PDF files, video clips,...)\r\n* `refine rules` - List of regular expressions will be applied to content to remove redundant data. Each regex stays in one different line.\r\n\r\n    *Example:*\r\n        \r\n        <div class=\"tags\".*$\r\n        <br/?>\r\n\r\n* `active` - Determine if this `source` will run or not\r\n* `download image` - Check this to download all images present inside the specified content\r\n* `black words` - Select set of words, a content will not be downloaded if containing one of those words\r\n* `proxy` - Proxy server will be used when crawling current source\r\n* `user agent` - User Agent value set in the header of every requests\r\n\r\nAfter being saved, the `source` object will run a scraping session by calling crawl() method:\r\n\r\n    source_object.crawl()\r\n\r\nor under console, by running management command `run_scraper`:\r\n    \r\n    python manage.py run_scraper\r\n    \r\nWith this command, all active sources inside current Django instance will be processed consecutively.\r\n\r\n--\r\n*For further information, issues, or any questions regarding this, please email to me[at]zniper.net*\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}